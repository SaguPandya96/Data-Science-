{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3- Information gain - mutual information In Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPKrjf/gn6CUt+aiyyrbwFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaguPandya96/Data-Science-/blob/master/3_Information_gain_mutual_information_In_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd4qNvsaA2cl"
      },
      "source": [
        "### **Feature Selection-Information gain - mutual information In Classification Problem Statements**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OMF3s2BBD3Q"
      },
      "source": [
        "### **Mutual Information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR2mgxSIBFz2"
      },
      "source": [
        "MI Estimate mutual information for a discrete target variable.\r\n",
        "\r\n",
        "Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\r\n",
        "\r\n",
        "The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\r\n",
        "\r\n",
        "Inshort\r\n",
        "\r\n",
        "A quantity called mutual information measures the amount of information one can obtain from one random variable given another.\r\n",
        "\r\n",
        "The mutual information between two random variables X and Y can be stated formally as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfg6-Mk0BJVF"
      },
      "source": [
        "### **I(X ; Y) = H(X) â€“ H(X | Y) Where I(X ; Y) is the mutual information for X and Y, H(X) is the entropy for X and H(X | Y) is the conditional entropy for X given Y. The result has the units of bits.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wL-za4gBRmp"
      },
      "source": [
        "### **Youtube Videos**\r\n",
        "\r\n",
        "Entropy: https://www.youtube.com/watch?v=1IQOtJ4NI_0\r\n",
        "\r\n",
        "Information Gain: https://www.youtube.com/watch?v=FuTRucXB9rA\r\n",
        "\r\n",
        "Gini Impurity: https://www.youtube.com/watch?v=5aIFgrrTqOw\r\n",
        "\r\n",
        "Statistical test: https://www.youtube.com/watch?v=4-rxTA_5_xA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hScQ10B6BWK3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7NPGAZcBOHw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}